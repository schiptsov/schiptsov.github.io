<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2022-12-09 Fri 22:16 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AI writing code</title>
<meta name="author" content="&lt;schiptsov@gmail.com&gt;" />
<meta name="generator" content="Org Mode" />
<link rel="preconnect" href="https://fonts.googleapis.com">

<link href="https://fonts.googleapis.com/css?family=Fira Sans" rel="stylesheet">

<link href="https://fonts.googleapis.com/css?family=Fira Code" rel="stylesheet">

<link rel='stylesheet' type='text/css' href='/css/main.css'/>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href=""> UP </a>
 |
 <a accesskey="H" href="index.html"> HOME </a>
</div><div id="content" class="content">
<header>
<h1 class="title">AI writing code</h1>
</header><p>
I am philosophically inclined guy instead of mathematically inclined,
which, at least to me, seems good. I can see things you, people, would
not believe.
</p>

<p>
There is a current hot meme that AI would write code. Well,
theoretically, just like an AI process could learn a suitable
<i>representation</i> for some repeatedly and consistently <i>observed</i> patterns,
it could learn a representation for a function or a whole small
algorithm, using standard optimization procedures and back propagation.
</p>

<p>
Feed it with desired (supervised) inputs and outputs and eventually,
after long enough trials and errors it may come up with a
<i>representation</i> which consists on Abstract syntax trees (not lines of
code) instead of an abstract weighted graph of a certain &ldquo;architecture&rdquo;.
</p>

<p>
There is nothing wrong with this proposal, after all, it is essentially
the same evolutionary process which yields <i>stable intermediate forms</i>
form a particular mixture of basic small molecules, being &ldquo;shacked&rdquo; long
enough.
</p>

<p>
<i>Emerging stable forms</i> is the key principle of almost everything, from
molecular biology and Life Itself, to strange social formations,
including memes.
</p>

<p>
But if we look carefully, all the stable forms seems to be already
emerged long ago, and the smartest guys at early MIT AI lab have noticed
this.
</p>

<p>
There are distinct patterns in molecular biology - linear sequences of
aminoacids with particular emergent electro-chenical properties, which arise out of
certain shapes produced by the spontaneous process of <i>protein folding</i>.
Nevertheless, a linear sequence is the most fundamental pattern or a
form.
</p>

<p>
What we call &ldquo;tree-like structures&rdquo; are another fundamental shape, and
individual synapses are such kind of phisical structures. Notice that
they are &ldquo;<i>directed</i>&rdquo; which means <i>both</i> structurally and as resulting
information flows.
</p>

<p>
<i>Lookup tables</i> are also there, and what we call an <i>enzyme</i>, which is a
molecular <i>machine</i> can be abstracted out as a &ldquo;pure function&rdquo;, or a
mechanical <i>procedure</i> which always produces the same output for the same
sets of inputs.
</p>

<p>
In other words, all thre fundamental stable building blocks are have
been emerged long ago, and smart people in good places have already
captured all the fundamental generalizations and abstracted them out. It
started with idealistic early LISPs and, in another school of thought,
with <i>Lambda Calculus</i>-based pure languages.
</p>

<p>
As you may know, there are only 3 fundamental patterns, out of which
<i>everything</i> can be produced - <i>sequences, branching and loops or
recursion</i>.
</p>

<p>
There are just 3 semantic forms enough for everything - <i>lambda</i>
abstraction, variable <i>binding</i> and <i>application</i>. Lambda, conceptually
at least, corresponds to an enzyme.
</p>

<p>
At a higher, system level, the fundamental principle is <i>coordination</i> based on
<i>asynchronous message passing</i>, and asynchronous is the key. This notion
has been captured and generalized in Erlang.
</p>

<p>
The &ldquo;agents&rdquo; are <i>reactive</i> (never proactive or predictive in principle)
and, again, fundamentally, maintain their own structural &ldquo;information&rdquo;
about how to act triggered by a &ldquo;message&rdquo; or a signal. This
&ldquo;information&rdquo; evolves separately.
</p>

<p>
Simple AI will never be able to &ldquo;learn&rdquo; multi-agent systems. It is a
different level of &ldquo;knowledge&rdquo; and evolution (of an &ldquo;organism&rdquo; or a &ldquo;society&rdquo;).
</p>

<p>
At the highest level of the &ldquo;knowledge&rdquo; how to act is preserved and
&ldquo;evolved&rdquo; in what we call a common, shared culture, and it is mostly in
a symbolically distilled form (capturing semantics of What Is).
</p>

<p>
Will AI discover any new fundamental basic forms? Certainly not. They have
already been discovered by evolution.
</p>

<p>
What then AI will &ldquo;write&rdquo;? Well, there is a good metaphor - the
fundamental cultural difference between  Indian and Japanese cooking.
Indians are basically throwing everything in a pot or a frying pan and
heat it up, while Japanese are trying to carefully select a perfectly matching sets of
ingredients, temperatures, durations and so on.
</p>

<p>
AI will write Indian-style implementations, just like the
representations it currently produces. Change a single node and
everything is completely distorted, which reflects the fact that the
representation is not even remotely optimal. On the contrary it is
sub-optimal and grossly redundant (without the information protecting
property) by definition.
</p>

<p>
The fundamental principle is that AI (a trial-and-error on steroids) is incapable of producing
a complex layered structure due to various fundamental constraints.
</p>

<p>
Essentially, it is due to the fact that a <i>depth-first search process</i>
once gone to a wrong &ldquo;branch&rdquo; will never return, keeping producing <i>vastly
sub-optimal</i> crap, which eventually will &ldquo;work&rdquo;.
</p>

<p>
A process analogous of <i>breadth-first search</i> require <i>more</i> than just feedback and
back-prop. It requires an adequate <i>representation or &ldquo;map&rdquo; of reality</i> - just what
the smartest people extracted from observing molecular biology, evolution
and life itself.
</p>

<p>
No AI is capable of doing extraction of <i>semantic knowledge</i>, similar to
arithmetic or a plane geometry, and without this capacity it will write
utter crap.
</p>

<p>
By the way, it is absolutely astonishing what LISP/ML family of
languages + Erlang has been able to generalize and abstract out. Very few people
understand.
</p>

<p>
So, do not listen to bullshitters, no matter how clever and fluent they
sound. Japanese-style cooking, or <i>proper semantic knowledge</i> will always win.
</p>

<p>
On the other hand, a <i>society</i> of communicating and competing intelligent
agents, each of which maintaining and refining its own representation of
the shared environment, never losing it after malfunction, and, perhaps, <i>broadcasting</i>
parts of it to all other agents, will, at least in theory, come up with
a structural shared &ldquo;knowledge&rdquo;, conceptually similar to Wikipedia, but
at the level of &ldquo;deeper&rdquo; that plain text.
</p>
</div>
<div id="postamble" class="status">
<p class="author">Author: &lt;schiptsov@gmail.com&gt;</p>
<p class="email">Email: <a href="mailto:lngnmn2@yahoo.com">lngnmn2@yahoo.com</a></p>
<p class="date">Created: 2022-12-09 Fri 22:16</p>
</div>
</body>
</html>