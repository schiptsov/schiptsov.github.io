<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2023-01-16 Mon 12:38 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>on AI</title>
<meta name="author" content="&lt;schiptsov@gmail.com&gt;" />
<meta name="generator" content="Org Mode" />
<link rel="preconnect" href="https://fonts.googleapis.com">

<link href="https://fonts.googleapis.com/css?family=Fira Sans" rel="stylesheet">

<link href="https://fonts.googleapis.com/css?family=Fira Code" rel="stylesheet">

<link rel='stylesheet' type='text/css' href='/css/main.css'/>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href=""> UP </a>
 |
 <a accesskey="H" href="index.html"> HOME </a>
</div><div id="content" class="content">
<header>
<h1 class="title">on AI</h1>
</header><p>
There are some universal principles about any kind of what we call
&ldquo;intelligence&rdquo;. There principles will show how far away we are from creating
anything similar to <i>General Artificial Intelligence</i> (but not something which mimiks
and cosplays it).
</p>

<p>
There is a fundamental, principal difference between something you <i>know</i> (either
from the experience of actually doing or by experience of going trough a formal
prof, or at least a correct outline of a proof).
</p>

<p>
When you just remembering something you have hard from others or even observed
others were doing, this does not account for (is not) a valid knowledge.
</p>

<p>
All current AI systems are vastly complex structures trained on mere words (a
model &ldquo;heard&rdquo; something others are saying) without constructing and validating
(through the means of a scientific method and formal logic) <i>verified conceptual
hierarchies</i>, which can be used as a valid <i>representation of knowledge</i> and being
used for decision making (just like any agent with an inner knowledge representation).
</p>

<p>
What we call &ldquo;language-based shared culture&rdquo; is not a synonym and not a valid
&ldquo;storage&rdquo; for knowledge (although it carries some along). <i>Memes</i> of various kinds
is what a shared culture carries along.
</p>

<ul class="org-ul">
<li>Every brain builds and maintains an inner representation of reality and
consults and uses it (and only it) for decision making and actions.</li>

<li>The &ldquo;machinery&rdquo; which builds and maintains this inner representation has been
evolved <i>to match the environment</i> in which it has been evolved.</li>

<li>All the <i>&ldquo;animal ingelligence&rdquo;</i> is of this way - they brain structure (and
an inner representation it maintains) has a  good-enough &ldquo;match&rdquo; of were they
are.</li>

<li>There is a certain pattern in software (and machine translation), where some
format (or encoding) first converted into a more general representation
(similar to an AST) and then from this representation another format or
encoding can be produced.</li>

<li>Human language-based communication works exactly the same way (the early NLP
guys of 80-s got it right) - it verbalizes brain&rsquo;s inner representation -
encodes &ldquo;knowledge&rdquo; for communication.</li>

<li>To be &ldquo;intelligent&rdquo; is to have the most adequate, the less distorted, of the
smallest possible distance from <i>What Is</i> inner representation of reality and to
use it.</li>

<li>The only way to build such representation&#x2026; well, it is complicated, but the
main principle is not to have any distortion in perception and experience.</li>

<li>This automatically rules out <i>almost</i> everything written and spoken, especially
on social media or internet forums, except, perhaps. very few definitive
textbooks of exact (and verifiable) sciences.</li>

<li>Once one have an adequate inner representation of certain aspects of reality
one can express or communicate &ldquo;knowledge&rdquo; in either <i>informal</i>, <i>formal</i> or
<i>programming</i> languages.</li>

<li>Programming languages require a very complex process of decomposition into
concepts, distilling of proper abstractions (both minimal and optimal), being
able to create in a <i>bottom-up</i> processes several layers of DSLs, producing a
layered conceptual hierarchy which mimics real hierarchies.</li>
</ul>

<p>
Then there is testing, verification and continuoud refactoring.
</p>

<p>
Now there is the catch. No adequate representation can be produced by an
unsupervised learning.
</p>

<p>
Period. No adequate representation can be produced by supervised learning of
<i>appearances</i> or <i>ready-made results</i>.
</p>

<p>
Only <i>mimicking</i> can be produced by mere observation of complex behavior.
</p>

<p>
<i>GPT</i> and <i>ChatGPT</i> mimiks sophisticated talking. They cannot produce an irrefutable
bottom-up argument based on the first principles. Only to search to find
something <i>similar</i> based on the &ldquo;weights&rdquo;.
</p>

<p>
And never, in principle, can a model understand the whole coneptual hierarchy
which leads ot the result it selects. This requires different kind of processes
to be evolved.
</p>

<p>
By merely looking at some notation or behavior it is impossible, again, in
principle, to get the underlying reasoning and coceptual hierarchies which
produced these observable results.
</p>

<p>
Exact sciences and concrete math guys know this.
</p>
</div>
<div id="postamble" class="status">
<p class="author">Author: &lt;schiptsov@gmail.com&gt;</p>
<p class="email">Email: <a href="mailto:lngnmn2@yahoo.com">lngnmn2@yahoo.com</a></p>
<p class="date">Created: 2023-01-16 Mon 12:38</p>
</div>
</body>
</html>