<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2022-12-16 Fri 17:44 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Deep Learning</title>
<meta name="author" content="&lt;schiptsov@gmail.com&gt;" />
<meta name="generator" content="Org Mode" />
<link rel="preconnect" href="https://fonts.googleapis.com">

<link href="https://fonts.googleapis.com/css?family=Fira Sans" rel="stylesheet">

<link href="https://fonts.googleapis.com/css?family=Fira Code" rel="stylesheet">

<link rel='stylesheet' type='text/css' href='/css/main.css'/>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href=""> UP </a>
 |
 <a accesskey="H" href="index.html"> HOME </a>
</div><div id="content" class="content">
<header>
<h1 class="title">Deep Learning</h1>
</header><p>
The fundamental principle behind <i>deep learning</i> (supervised and reinforcement) is that the &ldquo;right&rdquo; connections are getting &ldquo;reinforced&rdquo; while redundant or &ldquo;wrong&rdquo; ones are getting pruned.
</p>

<p>
The neuroscience meme &ldquo;wire together &#x2013; fire together&rdquo; has the same underlying principle.
</p>

<p>
Thus we lean a correct (undistorted) <i>representation</i> of some aspects  (recurrent patterns) of reality, which serves as an oversimplified but <i>principally correct &ldquo;map&rdquo;</i> of vastly complex and dynamic &ldquo;territory&rdquo;.
</p>

<p>
This <i>learned representation</i> is then used to <i>generate</i> other kinds of structured information, just like any good general converter between different formats does.
</p>

<p>
Learning a representation of a &ldquo;function&rdquo; or a transformation is a special case, and there is nothing magical about it. Just like a muscle &ldquo;learns&rdquo; from repeated and regular actions. It is, by the way, a correct metaphor to say that a brain is also a set of &ldquo;muscles&rdquo;.
</p>

<p>
What we call &ldquo;learning&rdquo; is, in general, a trial-and-error with a feed-back loop process, which, at least in theory, will eventually reach its &ldquo;fixed point&rdquo;. 
</p>

<p>
Feedback, which is implemented as <i>back propagation algorithm</i> is the key to everything &#x2013; it guarantees at least some convergence, or a &ldquo;perfect match&rdquo; (but not necessary a global optimum).
</p>

<p>
Just like it is with biological systems, learning occur <i>continuously</i> through repeated and regular exposure to <i>consistent</i> (non-selfcontradictory) &ldquo;stable&rdquo; environment.
</p>

<p>
Biological systems, in theory, require no supervision (although parents and games are essential) because the <i>environmental constraints</i> act as &ldquo;filters&rdquo; of possible experience, so the <i>representation</i> end up perfectly matching the environment in which it evolves.
</p>

<p>
The crucial part is that biological systems accumulate (via random <i>favourable</i> mutations) and transmit &ldquo;knowledge&rdquo; by encoding a better &ldquo;structures&rdquo; of brain regions in the DNA. This is how vision or hearing came to be. Language. Everything.
</p>

<p>
You won&rsquo;t believe, but that is it. Almost every actual application of AI, including AI-generated hentai and what not, are just these principles being applied and implemented in code.
</p>

<p>
NNs weights starts at random, while nature uses evolved structure, just like a &ldquo;template&rdquo;. A language area does not start as a random mesh of neurons, it is pre-arranged by evolution, just like any other specialized brain area.
</p>

<p>
Deep learning mimics an evolved way of simple <i>pattern-recognition</i> in a stable environments (where these patterns are recurrent and stable).
</p>

<p>
This view of <i>deep learning</i> is <i>less wrong</i>, closer to actual reality. Any references to &ldquo;intelligence&rdquo; are just bullshit. Pattern recognition is an important first step, but intelligence is far from it.
</p>

<p>
An &ldquo;animal&rdquo; intelligence requires awareness, consciousness capable of introspection (of its emotional states). Higher (human) intelligence require self-consciousness and language-based abstract thoughts.
</p>

<p>
Again, deep learning in <i>unstable environments</i> is modern day&rsquo;s alchemy and tantra.
</p>

<p>
On my old now defunct cite I had a definitive example of how it works.
</p>

<p>
Butterflies have evolved eye-mimicking spots on their wings without being aware of it, of course, <i>because in the stable shared environment</i> creatures do have eyes.
</p>

<p>
This is not just being selected by evolution, it &ldquo;works&rdquo; because other creatures recognize these patterns as eyes and stay away. Notice that this is noting but <i>pattern-recognition</i> at work.
</p>

<p>
Since the Upanishads we know that any knowledge is based on experience, end our deductive or inductive reasoning also begins with something &ldquo;real&rdquo;, grounded in reality. At least this is how it should be.
</p>

<p>
But the brain is just layers upon layers of vastly complex, specialized neural networks. The major areas (cortexes) has been evolved to have a particular structure but this structure begins as a template, which get refined with learning by experience (by doing).
</p>

<p>
This implies that we learn what we have been exposed to, and, necessarily, the constraints of the shared environment, which, by the way, reflected in the structures of our brains (the vision system is not arbitrary and evolved to match this particular planet).
</p>

<p>
The point is to show that ancient philosophy of mind is still valid and even applicable to artificial neural networks, whoever crude they may be.
</p>

<p>
Any set of algorithms, supervised and/or reinforced, will learn to match patterns to which it has been exposed (without making any inductive or deductive steps, which requires abstract thinking and language).
</p>

<p>
The big idea is that the principles of learning and of using neural nets to refine the current representation are the same for all brains.
</p>

<p>
This also implies that being exposed to (or supervised with) an inconsistent &ldquo;random&rdquo; bullshit will inevitably yield bullshit, so neural nets could recognize what isn&rsquo;t there, just like sick people. This explains repeated utter failures of applying deep learning to &ldquo;abstract&rdquo; domains.
</p>

<p>
The validation tests on a new data will always fail, because the environment has been evolved since then.
</p>

<p>
Justification of this claim is that <i>the knowledge is in the weights</i> (in current internal states of neurons).
</p>
</div>
<div id="postamble" class="status">
<p class="author">Author: &lt;schiptsov@gmail.com&gt;</p>
<p class="email">Email: <a href="mailto:lngnmn2@yahoo.com">lngnmn2@yahoo.com</a></p>
<p class="date">Created: 2022-12-16 Fri 17:44</p>
</div>
</body>
</html>