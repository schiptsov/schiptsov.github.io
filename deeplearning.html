<!DOCTYPE html>
<html lang="en">
<head>
  <!-- 2022-09-29 Thu 19:33 -->
  <meta charset="utf-8">
  <meta name="viewport" content=
  "width=device-width, initial-scale=1">
  <title>Deep Learning</title>
  <meta name="author" content="schiptsov@gmail.com">
  <meta name="generator" content="Org Mode">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css?family=Fira Sans"
  rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Fira Code"
  rel="stylesheet">
  <link rel='stylesheet' type='text/css' href='/css/main.css'>
</head>
<body>
  <div id="org-div-home-and-up">
    <a accesskey="h" href="">UP</a> | <a accesskey="H" href=
    "index.html">HOME</a>
  </div>
  <div id="content" class="content">
    <header>
      <h1 class="title">Deep Learning</h1>
    </header>
    <p>The fundamental principle behind <i>deep learning</i>
    (supervised and reinforcement) is that the “right” connections
    are getting “reinforced” while redundant or “wrong” ones are
    getting pruned.</p>
    <p>The neuroscience meme “wire together - fire together” has
    the same underlying principle.</p>
    <p>NNs weights starts at random, while nature uses evolved
    structure, just like a “template”. A language area does not
    start as a random mesh of neurons, it is pre-arranged by
    evolution, just like any other specialized brain area.</p>
    <p>Deep learning mimics an evolved way of simple
    <i>pattern-recognition</i> in a stable environments (where
    these patterns are recurrent and stable).</p>
    <p>This view of <i>deep learning</i> is <i>less wrong</i>,
    closer to actual reality. Any references to “intelligence” are
    just bullshit. Pattern recognition is an important first step,
    but intelligence is far from it.</p>
    <p>An “animal” intelligence requires awareness, consciousness
    capable of introspection (of its emotional states). Higher
    (human) intelligence require self-consciousness and
    language-based abstract thoughts.</p>
    <p>Again, deep learning in <i>unstable environments</i> is
    modern day’s alchemy and tantra.</p>
    <p>On my old now defunct cite I had a definitive example of how
    it works.</p>
    <p>Butterflies have evolved eye-mimicking spots on their wings
    without being aware of it, of course, <i>because in the stable
    shared environment</i> creatures do have eyes.</p>
    <p>This is not just being selected by evolution, it “works”
    because other creatures recognize these patterns as eyes and
    stay away. Notice that this is noting but
    <i>pattern-recognition</i> at work.</p>
    <p>Since the Upanishads we know that any knowledge is based on
    experience, end our deductive or inductive reasoning also
    begins with something “real”, grounded in reality. At least
    this is how it should be.</p>
    <p>But the brain is just layers upon layers of vastly complex,
    specialized neural networks. The major areas (cortexes) has
    been evolved to have a particular structure but this structure
    begins as a template, which get refined with learning by
    experience (by doing).</p>
    <p>This implies that we learn what we have been exposed to,
    and, necessarily, the constraints of the shared environment,
    which, by the way, reflected in the structures of our brains
    (the vision system is not arbitrary and evolved to match this
    particular planet).</p>
    <p>The point is to show that ancient philosophy of mind is
    still valid and even applicable to artificial neural networks,
    whoever crude they may be.</p>
    <p>Any set of algorithms, supervised and/or reinforced, will
    learn to match patterns to which it has been exposed (without
    making any inductive or deductive steps, which requires
    abstract thinking and language).</p>
    <p>The big idea is that the principles of learning and of using
    neural nets to refine the current representation are the same
    for all brains.</p>
    <p>This also implies that being exposed to (or supervised with)
    an inconsistent “random” bullshit will inevitably yield
    bullshit, so neural nets could recognize what isn’t there, just
    like sick people. This explains repeated utter failures of
    applying deep learning to “abstract” domains.</p>
    <p>The validation tests on a new data will always fail, because
    the environment has been evolved since then.</p>
    <p>Justification of this claim is that <i>the knowledge is in
    the weights</i> (in current internal states of neurons).</p>
  </div>
  <div id="postamble" class="status">
    <p class="author">Author: schiptsov@gmail.com</p>
    <p class="email">Email: <a href=
    "mailto:lngnmn2@yahoo.com">lngnmn2@yahoo.com</a></p>
    <p class="date">Created: 2022-09-29 Thu 19:33</p>
  </div>
</body>
</html>
